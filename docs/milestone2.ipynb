{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvAtKturiXPv"
      },
      "source": [
        "<h1 align=\"center\">AD-fbi: Automatic Differentiation Python Package</h1>\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Our software package, *AD-fbi*, computes gradients using the technique of automatic differentiation. Automatic differentiation is important because it is able to solve the derivatives of complex functions at low cost while maintaining accuracy and stability. Its high practicality and precision make it applicable to a wide range of tasks, such as machine learning optimization, calculating financial loss or profit, and minimizing the cost of vaccine distribution. Automatic differentiation plays a crucial role in solving these problems. \n",
        "\n",
        "Prior to the technique of automatic differentiation, the two traditional computational techniques for solving differential equations are numerical differentiation and symbolic differentiation. The numerical differentiation method computes derivatives based on a finite numerical approximation which lacks stability and precision, and thus it is not ideal to be implemented in computer software. On the other hand, symbolic differentiation can become very computationally expensive as the function becomes more and more complex. \n",
        "\n",
        "The automatic differentiation technique transforms the entire computational process into a sequence of elementary arithmetic operations -- addition, subtraction, multiplication, division, and elementary functions. And then, it applies the chain rule on top of these elementary operations. In this way, it reduces the computational complexity that the model can reuse some of its parts computed in previous steps multiple times without re-calculating them. It keeps the same level of accuracy and precision as using symbolic differentiation. \n",
        "\n",
        "Our software package computes derivatives using the forward mode of auto differentiation and will later explore other extensions (different optimizer). \n",
        "\n",
        "\n",
        "# Background\n",
        "* __Chain Rule__\n",
        "\n",
        "Chain Rule is the most important concept in automatic differentiation. To solve the derivatives of a composite function, we use the Chain Rule to decompose each variable in the function into elementary components and mulptily them together. \n",
        "\n",
        "For a given function $f(y(x))$, the derivative of $f$ with respect to $ x $ is the following:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial x}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Since $y$ is a n-dimensional vector, we introduce the gradient operator $ \\nabla $ into the expression to calculate the derivative of $y$ with respect to $x$, where $x = (x_1, ..., x_n)$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\nabla y(x) =\n",
        "\\begin{bmatrix}\n",
        "{\\frac {\\partial y}{\\partial x_{1}}}(x)\n",
        "\\\\\n",
        "\\vdots \n",
        "\\\\\n",
        "{\\frac {\\partial y}{\\partial x_{n}}}(x)\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The above expression is for a single $y$, but we typically have multiple $y$ in a neural network. Thus, for a given function $f(y_1(x), ..., y_n(x))$, the derivative of $f$ with respect to $x$ is defined as the following:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\nabla f_x = \\sum_{i=1}^n \\frac{\\partial f}{\\partial y_i} \\nabla y_i(x)\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "* __Forward Mode automatic differentiation__\n",
        "\n",
        "Here we will give an example of how to do forward mode automatic differentiation.\n",
        "\n",
        "Given x = $\\begin{bmatrix} {x_1} \\\\ \\vdots \\\\ {x_m} \\end{bmatrix}$, where $k \\in (1, 2, ..., m)$, we introduce the intermediate operations $ v $ to compute values at each elementary operation step.\n",
        "\n",
        "For example, to compute the gradient $\\nabla f$  of the function $f(x) = log(x_1) + sin(x_1 + x_2)$, the expression is derived as the following:\n",
        "\n",
        "$\\nabla f = \\begin{bmatrix} \\frac {\\partial f} {\\partial x_1} \\\\ \\frac {\\partial f} {\\partial x_2} \\end{bmatrix}  = \\begin{bmatrix} \\frac {1} {x_1} + \\cos(x_1 + x_2) \\\\ \\cos(x_1 + x_2) \\end{bmatrix}$\n",
        "\n",
        "The computation graph is shown here: \n",
        "![alt text](graph.png \"Computational Graph\")\n",
        "\n",
        "$D_p v_{-1} = \\nabla v_{-1}^T p = (\\frac {\\partial v_{-1}} {\\partial x_1} \\nabla x_{1})^T p = (\\nabla x_{1})^T p = p_1$\n",
        "\n",
        "$D_p v_{0} = \\nabla v_{0}^T p = (\\frac {\\partial v_{0}} {\\partial x_2} \\nabla x_{2})^T p = (\\nabla x_{2})^T p = p_2$\n",
        "\n",
        "$D_p v_{1} = \\nabla v_{1}^T p = (\\frac {\\partial v_{1}} {\\partial v_{-1}} \\nabla v_{-1} + \\frac {\\partial v_{1}}{\\partial v_{0}} \\nabla v_{0})^T p = (\\nabla v_{-1} + \\nabla v_0)^T p = D_p v_{-1} + D_p v_0$\n",
        "\n",
        "$D_p v_{2} = \\nabla v_{2}^T p = (\\frac {\\partial v_{2}} {\\partial v_{1}} \\nabla v_1)^T p = \\cos(v_1) (\\nabla v_1)^T p = \\cos(v_1) D_p v_1$\n",
        "\n",
        "$D_p v_{3} = \\nabla v_{3}^T p = (\\frac {\\partial v_{3}} {\\partial v_{-1}} \\nabla v_{-1})^T p = \\frac {1} {v_{-1}} (\\nabla v_{-1})^T p = \\frac {1} {v_{-1}} D_p v_{-1}$\n",
        "\n",
        "$D_p v_{4} = \\nabla v_{4}^T p = (\\frac {\\partial v_{4}} {\\partial v_3} \\nabla v_{3} + \\frac {\\partial v_{4}}{\\partial v_{2}} \\nabla v_{2})^T p = (\\nabla v_{3} + \\nabla v_2)^T p = D_p v_{3} + D_p v_2$\n",
        "\n",
        "Thus, the final generalized formula is the following:\n",
        "\n",
        "$$ D_p v_j = (\\nabla v_j)^T p = (\\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} \\nabla v_i)^T p = \\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} (\\nabla v_i)^T p = \\sum_{i < j} \\frac{\\partial{v_j}} {\\partial{v_i}} D_p v_i$$ \n",
        "\n",
        "\n",
        "* __Jacobian Matrix__\n",
        "\n",
        "Having derived the above system of equations, we want to use the Jacobian matrix to compute these derivatives systematically.\n",
        "\n",
        "The Jacobian matrix is defined as the following:\n",
        "\n",
        "$$\n",
        "J_{p}(x_{1},x_{2}, ..., x_{n}) = \\left[ \\begin{matrix}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & ... & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n",
        "\\vdots  & \\ddots & \\vdots  \\\\\n",
        "\\frac{\\partial y_{m}}{\\partial x_{1}} & ... & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{matrix} \\right] \n",
        "$$\n",
        "\n",
        "\n",
        "For example, a 2 by 2 Jacobian matrix with 2 variables looks like the following:\n",
        "\n",
        "$$\n",
        "J_{p}(x_{1},x_{2}) = \\left[ \\begin{matrix}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} \\\\\n",
        "\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}}\n",
        "\\end{matrix} \\right] \n",
        "$$\n",
        "\n",
        "We compute $J_{p}$ in the forward mode in the evaluation trace.\n",
        "\n",
        "* __Seed Vector__\n",
        "\n",
        "Seed vector $p$ provides an efficient way to retrieve elements in a given direction from the Jacobian matrix. For example, if we want to retreive the $i$, $j$ element from the Jacobian matrix, the seed vector $p = [\\overrightarrow{i}, \\overrightarrow{j}]$ helps to retrieve the element of $\\frac{df_{i}}{dx_{j}}$. \n",
        "\n",
        "We will introduce the seed vector in the forward trace to facilitate the retrieval of any element in the Jacobian matrix to make the calculation process more efficient and faster. The default value of the seed vector is 1\n",
        "\n",
        "\n",
        "# How to use *AD-fbi*\n",
        "\n",
        "1. __Installing the package:__\n",
        "\n",
        "   * install our package with the following line: \n",
        "    \n",
        "    <code>python3 -m pip install AD-fbi </code> (NOT IMPLEMENTED)\n",
        "\n",
        "   * clone our github repository with the following line: \n",
        "\n",
        "    <code>git clone https://code.harvard.edu/CS107/team06.git</code>\n",
        "\n",
        "   * cd to team06 directory: \n",
        "\n",
        "    <code>cd team06</code>\n",
        "  \n",
        "   * create a virtual environment:\n",
        "\n",
        "    <code>python3 -m venv fbi-env</code>\n",
        "\n",
        "   * install dependencies from requirement.txt: \n",
        "\n",
        "    <code>python3 -m pip install -r requirement.txt</code> \n",
        "\n",
        "   * cd to source directory: \n",
        "\n",
        "    <code>cd src/</code>\n",
        "\n",
        "   * open python console:\n",
        "  \n",
        "    <code>python3</code>\n",
        "\n",
        "\n",
        "2. __Importing the package:__\n",
        "\n",
        "   * import our package using python console:\n",
        "    \n",
        "    ```python\n",
        "    from fbi.forward_mode import ForwardMode\n",
        "    ```\n",
        "    \n",
        "3. __Instantiating AD objects:__\n",
        "\n",
        "  3.1. Instantiate an AD object for a scalar function of a single input ($R$ -> $R$):\n",
        "   * Input: \n",
        "    - <code>func</code>: function $f(x)$ \n",
        "    - <code>x</code>: a single value to be calculated with respect to $f(x)$ <code>func</code>\n",
        "    - <code>seed</code>: seed vector\n",
        "  \n",
        "**Example:**\n",
        "  * Define variables\n",
        "  ```python\n",
        "  func = lambda x: x + 1\n",
        "  x, seed = 1, -1\n",
        "  fm = ForwardMode(x, func, seed)\n",
        "  ```\n",
        "\n",
        "  * To calculate function value and dual value with respect to <code>func</code>:\n",
        "   ```python\n",
        "  print(fm.calculate_dual_number())\n",
        "  ```\n",
        "  output:\n",
        "  ```\n",
        "  >>> (2, -1)\n",
        "  ```\n",
        "\n",
        "  * To calculate $f(x)$:\n",
        "   ```python\n",
        "  print(fm.get_fx_value())\n",
        "  ```\n",
        "  output:\n",
        "  ```\n",
        "  >>> 2 \n",
        "  ```\n",
        "\n",
        "  * To calculate derivative with respect to $f(x)$:\n",
        "   ```python\n",
        "  print(fm.get_derivative())\n",
        "  ```\n",
        "  output:\n",
        "  ```\n",
        "  >>> -1 \n",
        "  ```\n",
        " \n",
        "3.2. (TO BE IMPLEMENTED) Instantiate an AD object for a vector function with multiple input variables ($R^m$ -> $R^n$): \n",
        "\n",
        "   ```python\n",
        "   # input variable -  a vector of R^3\n",
        "   x = np.array([0.5, 0.5, 0.2]) \n",
        "\n",
        "   # output function -  R^3 -> R^2\n",
        "   f_x = lambda x, y, z: (x.log() + y + z, x * y + z)\n",
        "\n",
        "   # instantiate an AD object\n",
        "   fm = ad.ForwardMode(evaluate = x, function = f_x, seed=np.array([1,2,3]))\n",
        "   ```\n",
        "\n",
        "3.3. (TO BE IMPLEMENTED) Instantiate an AD object for a vector function with multiple input variables ($R^m$ -> $R$):\n",
        "\n",
        "   ```python\n",
        "   # input variable -  a vector of R^3\n",
        "   x = np.array([0.5, 0.5, 0.2]) \n",
        "\n",
        "   # output function -  R^3 -> R\n",
        "   f_x = lambda x, y, z: x * y + z\n",
        "\n",
        "   # instantiate an AD object\n",
        "   fm = ForwardMode(x, f_x, np.array([1,2,3]))\n",
        "   ```\n",
        "\n",
        "3.4. (TO BE IMPLEMENTED)  __Adam Optimizer:__\n",
        "   We aim to develop optimizers such as the Adam optimizer for stochastic gradient descent to find the minima of a function. \n",
        "   Here is an example of how to interact with the feature.\n",
        "   \n",
        "   ```python\n",
        "   from AD-fbi import optimization\n",
        "\n",
        "   x = 1\n",
        "   \n",
        "   # define the function to find the minima points\n",
        "   f_x = lambda x: x**3 + 3 * x\n",
        "\n",
        "   # instantiate different optimizers\n",
        "   opm_adam = Optimization.adam()\n",
        "   opm_mom = Optimization.momentum()\n",
        "   opm_ada = Optimization.ada()\n",
        "\n",
        "   # find the mimina points and the running times for each optimizer\n",
        "   # the optimizer methods will return the running time, the minimum values of the function (minima), and the locations to get the minima\n",
        "   time_adam, min_adam, x_vals_adam = opm_adam(x, f_x, num_iter=100, epsilon=1e-10)\n",
        "   time_mom, min_mom, x_vals_mom = opm_mom(x, f_x, num_iter=100, epsilon=1e-10)\n",
        "   time_ada, min_ada, x_vals_ada = opm_ada(x, f_x, num_iter=100, epsilon=1e-10)\n",
        "\n",
        "   ```\n",
        "\n",
        "\n",
        "# Software Organization\n",
        "\n",
        "__Folder structure__\n",
        "\n",
        "The folder structure for the *AD-fbi* package:\n",
        "\n",
        "```\n",
        "team06/\n",
        "      docs/\n",
        "        graph.png\n",
        "        directory_tree.png\n",
        "        milestone1.ipynb\n",
        "        milestone2_progress.md\n",
        "        documentation.ipynb\n",
        "      src/\n",
        "        fbi/\n",
        "          __init__.py\n",
        "          dual_number.py\n",
        "          forward_mode.py\n",
        "        tests/\n",
        "          __init__.py\n",
        "          test_dual_number.py\n",
        "          test_forward_mode.py\n",
        "      requirements.txt\n",
        "      setup.py (TO BE IMPLEMENTED)\n",
        "      setup.cfg (TO BE IMPLEMENTED)\n",
        "      README.md\n",
        "      LICENSE\n",
        "``` \n",
        "\n",
        "__Modules & Functionalities__\n",
        "\n",
        "The AD-fbi package contains three essential modules: 1) a dual number module for elementary operations, 2) forward mode automatic differentiation, and 3) an Adam optimizer extension module. The followings are the decriptions of their functionalities:\n",
        "\n",
        "   * dual_number.py: This module implements the DualNumber class, which contains methods to compute the function and derivative values of the elementary operations, and to overload the elementary operations for a dual number. These functions are essential to computing the primal and tangent trace in the forward mode of AD. Examples of elementary operations include: <code>+</code>, <code>-</code>, <code>\\*</code>, <code>/</code>, <code>sin()</code>, <code>cos()</code>.\n",
        "    \n",
        "   * forward_mode.py: This module implements a ForwardModel class that provides a method to intialize a forward mode AD object, a method to construct a computational graph dictionary, and a method to run the forward mode process and returns the computed derivative of the function at the evaluated point.\n",
        "\n",
        "   * optimization.py (extension module TO BE IMPLEMENTED): This module implements the Adam optimizer for stochastic gradient descent to facilitate the basic automatic differentiation functionalities. \n",
        "\n",
        "__Test Suite__\n",
        "\n",
        "The test suite lives in the <code>team06/src/tests/</code> directory. It contains all pytests for our package. The YAML file under the <code>team06/.github/workflows/</code> directory defines the CI setup and jobs for GitHub Actions. When a new code is pushed to main branch, all tests are automatically trigered to test the code. \n",
        "\n",
        "__Package Distribution__\n",
        "\n",
        "The package is distributed via PyPI. We first add a _pyproject.toml_ file to our project, then install `build` (a PEP517 package builder). After that, we build our package release and finally upload it to PyPI. We also add the setup.py and setup.cfg files to set up the backend of our package development. (TO BE IMPLEMENTED)\n",
        "\n",
        "Users can install the package using the following command line:\n",
        "```\n",
        "python3 -m pip install AD-fbi\n",
        "```\n",
        "Install dependencies using the following command line:\n",
        "```\n",
        "python3 -m pip install -r requirements.txt\n",
        "```\n",
        "NOTE: Our package has not uploaded to PyPI at this point. You can clone our github repository and follow the instruction under the **How to use AD-fbi** section. \n",
        "\n",
        "__Package Dependencies__\n",
        "\n",
        "We rely on the following external dependencies in our package:\n",
        "* numpy==1.22.0\n",
        "* pytest==7.1.2\n",
        "* pytest_cov==4.0.0\n",
        "\n",
        "\n",
        "# Implementation\n",
        "    \n",
        "* __Main Classes__\n",
        "  * <code>DualNumbers</code>: class for operations with a dual number.\n",
        "  * <code>ForwardMode</code>: class for forward mode differentiation.\n",
        "  * <code>AdamOptimizer</code>(extension module TO BE IMPLEMENTED): class for the adam optimizer. This object has no attributes, but each method within the\n",
        "   class requires a <code>x</code> input, a function <code>func</code> input, and the number of iterations <code>num_iter</code> for the specific optimizing method. Additionally, each method\n",
        "   has their own optional hyperparameters which the user can input if they choose not to use our standard default values.\n",
        "\n",
        "* __Core Data Structures & Dual Numbers__\n",
        "  * Our primary core data structure is a numpy array, which we use to store both the variable list and the function list. Then using the\n",
        "   methods within the `ForwardMode` class, we compute the jacobian and function value. We store the corresponding values or arrays in a tuple. For a single input, the jacobian and function value are singular values, and for multi-dimensional vector input, the stored values are arrays.\n",
        "\n",
        "\n",
        "* __Classes Method & Name Attributes__\n",
        "  * <code>DualNumbers</code>: \n",
        "    * A `__init__` method to initialize a <code>DualNumbers</code> object with a real number value and a dual number value.\n",
        "    * A `__repr__` method to return the object representation in string format.\n",
        "    * Multiple methods to overload the elementary operations for a dual number. e.g. `__add__`, `__sub__`, `__mul__`, `__div__`, `__pow__`, `__radd__`, `__rsub__`, `__rmul__`, `__rdiv__`, `__rpow__`, etc.\n",
        "    * Multiple methods to compare dual numbers. e.g. `__ne__`, `__eq__`, etc.\n",
        "    * Multiple methods to transform a dual number. e.g. `sqrt`, `log`, `sin`, `cos`, `exp`, `tan`, etc.\n",
        "\n",
        "  * <code>ForwardMode</code>:\n",
        "    * A `__init__` method  to initialize a `ForwardMode` Object with an input value `x`, a function `func`, and a seed vector `seed`.\n",
        "    * A `get_fx_value` method to run the forward mode process and return a function value at the evaluated point `x`.\n",
        "    * A `calculate_dual_number` method to run the forward mode process and return the evaluated value and the derivative of the input function at the evaluated point `x`.\n",
        "    * A `get_derivative` method to run the forward mode process and return a value of directional derivative corresponding to a specific seed vector.\n",
        "\n",
        "* __Graph Class__\n",
        "  * We do not use a graph class to resemble the computational graph in forward mode. \n",
        "  \n",
        "* __Dealing With Operator Overloading and Elementary Functions__\n",
        "\n",
        "   * For the overloading operator template (like `__add__` for our special dual number class object), we implemented multiple methods such as `__radd__`, `__rsub__`, `__rmul__`, etc. to handle both cases of dual number + integer, and integer + dual number. The `__r*__` methods are necessary to handle overloading. \n",
        "\n",
        "   *  As listed above, within the <code>DualNumbers</code> class we’ve overloaded the simple arithmetic functions (addition, subtraction, multiplication,\n",
        "   division, negation, power, equal, and not equal) to calculate both the value and the dual number. We’ve also defined our own\n",
        "   elementary functions, such as `sin` and `sqrt` etc. to compute the value and the derivative. This module\n",
        "   generalizes each of the functions in order for the <code>ForwardMode</code> class to handle both scalar and vector inputs. Each method has also implemented raise error attribute to deal with all possible types of invalid inputs. The output is a tuple of the function value and the derivative, which\n",
        "   is used in the <code>ForwardMode</code> class.\n",
        "   \n",
        "* __Dealing With Operator Overloading on Reverse Mode__\n",
        "\n",
        "   * We currently are not interested in implementing a reverse mode on our package.\n",
        "\n",
        "* __Dealing With MultiDimensional Input and Output Function__\n",
        "\n",
        "   * Use `try` and `except` to handle multi-dimensional and single-dimensional case separetly. For the multi-dimensional case, we design a helper function to loop through all of the functions' inputs and reassign the value/derivative as a vector. \n",
        "   * We treat functions as a list (so high dimensional functions will be a list of functions)\n",
        "   * The `grad()` function (or jacobian function) is generic to both single-dimensional and multi-dimensional(as they are either a list of 1 or a list of mulitple functions).\n",
        "   \n",
        "* __External Dependencies__\n",
        "   * We use the numpy library to create our data structure for the computational graph and perform\n",
        "   computations outside of those we created in our dual_number class.\n",
        "\n",
        "# License\n",
        "\n",
        "Our *AD-fbi* package is licensed under the GNU General Public License v3.0. This free software license allows users to do just about anything they want with our project, except distribute closed source versions. This means that any improved versions of our package that individuals seek to release must also be free software. We find it essential to allow users to help each other share their bug fixes and improvements with other users. Our hope is that users of this package continually find ways to improve it and share these improvements within the broader scientific community that uses automatic differentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJQq6vRb0_oV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cba14c60c259a8ec8052f2b34631c57a7dce864bd9eed4f9529c2280f012d87a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
